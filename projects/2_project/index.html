<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Computational Learning in Games | Nicolas Eschenbaum</title> <meta name="author" content="Nicolas Eschenbaum"> <meta name="description" content="U of St. Gallen and Hasler Foundation funded work on RL in economic games"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700%7CRoboto+Slab:100,300,400,500,700%7CMaterial+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://neschenbaum.github.io/projects/2_project/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Nicolas </span>Eschenbaum</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">consulting</a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">research</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Computational Learning in Games</h1> <p class="post-description">U of St. Gallen and Hasler Foundation funded work on RL in economic games</p> </header> <article> <p>Companies increasingly employ machine learning techniques in their decision-making. In digital markets in particular, such algorithms are a key factor in ensuring a firm’s success. But despite their widespread adoption, we know very little about how algorithms interact and how they can influence economic outcomes. In this project, we investigate the behavior of interacting reinforcement learning algorithms in standard economic games and develop a software environment in which the behavior of learners in mathematical games can be studied. </p> <p>Funding by the University of St. Gallens Basic Research Fund allowed us to lay the groundwork by implementing simple Q-learning algorithms in repeated pricing games with imperfect competition. In <a href="https://neschenbaum.github.io/assets/pdf/robust-algorithmic-collusion.pdf">Eschenbaum et al. (2021)</a>, we apply our software to study the ability of learners to tacitly collude in Bertrand-style games. A recent literature finds that algorithms consistently learn to collude, however existing work reports results solely based on performance in the training environment. In practice, firms train their algorithms offline before using them, and the training and market environment differ. To study the possibility for learners to extrapolate collusive policies to the market environment, we develop a formal, general framework to guide the analysis of reinforcement learners in economic games.</p> <p>We first show that while algorithmic collusion consistently arises during training, in testing contexts collusion vanishes. This does not change with further iterations. Instead, we observe evidence of Nash play of the underlying stage game. This shows that algorithmic collusion is not robust to changes in the environment, and performance during training is no indication of outcome in the market.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <img class="img-fluid rounded z-depth-1" src="/assets/img/algos-main-result.png" alt="" title="Profits in training vs testing contexts"> </div> <div class="col-sm mt-3 mt-md-0"> <img class="img-fluid rounded z-depth-1" src="/assets/img/algos-transitive-closure-1.png" alt="" title="An example of a strategy pair"> </div> </div> <div class="caption"> On the left, average profits measured using a collusion index in training and testing contexts. On the right, the transitive closure of a strategy pair after convergence during training. Source: <a href="https://neschenbaum.github.io/assets/pdf/robust-algorithmic-collusion.pdf">Eschenbaum et al. (2021)</a>. </div> <p>It turns out that the reason for this breakdown of collusion is an overfitting to rival strategies from training, and more broadly the high-dimensionality of the strategy space. Even pairs of jointly converged policies that lead to the same collusive outcome during training can be vastly different, and all generally have multiple different stable outcomes that can be played. It is unsurprising that a given policy learnt during training then does not lead to the same outcome when playing against a policy different to the rival policy from training.</p> <div class="row justify-content-sm-center"> <div class="col-sm-6"> As a consequence, we further show that restricting algorithms' strategy space can solve this problem and make algorithmic collusion robust. Outcomes during training and testing now become indistinguishable, and for part of the space of parameters that we consider, we obtain collusive outcomes. The restriction on the strategy space can make algorithmic collusion robust, because it forces algorithms to learn collusive policies based on simpler patterns that are not too specific to the training context and can thus be extrapolated. </div> <div class="col-sm-6 mt-3 mt-md-0"> <img class="img-fluid rounded z-depth-1" src="/assets/img/algos-restrict-result.png" alt="" title="Profits when the strategy space is restricted"> </div> </div> <p>Our analysis suggests that a key driving force of algorithmic collusion in practice is coordination of algorithm <em>design</em>. Firms in practice may be able to successfully achieve algorithmic (tacit) collusion by coordinating on high-level ideas for the implementation of learning algorithms. </p> <p>Further funding by the Hasler Foundation has allowed us to develop a comprehensive integration of game theory and machine learning. We have integrated a programmatic, modular framework of mathematical games called open games with machine learning libraries, in particular Ray and Rlib. The repository for open games can be found <a href="https://github.com/philipp-zahn/open-games-hs" rel="external nofollow noopener" target="_blank">here</a>, for the underlying re-development of game theory called compositional game theory see <a href="https://arxiv.org/abs/1603.04641" rel="external nofollow noopener" target="_blank">Ghani et al, 2018</a>. This software allows a modular construction and combination of complex games with separately scripted cutting-edge RL algorithms.</p> <h4>Project materials:</h4> <ul> <li> Nicolas Eschenbaum, Filip Mellgren, and Philipp Zahn. <a href="/assets/pdf/robust-algorithmic-collusion.pdf">Robust Algorithmic Collusion</a> (WP). <i> 2021.</i> </li> </ul> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2023 Nicolas Eschenbaum. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>